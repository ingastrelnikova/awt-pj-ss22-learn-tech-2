{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "663531e1",
   "metadata": {},
   "source": [
    "# Competencies Extraction Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e84da09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install spacy\n",
    "# python -m spacy download de_core_news_lg\n",
    "# pip install keybert\n",
    "# pip install spacy-universal-sentence-encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d400ed",
   "metadata": {},
   "source": [
    "##  Tools and Libaries\n",
    "pandas: a software library written for the Python programming language for data manipulation and analysis\n",
    "\n",
    "spacy: an open-source software library for advanced natural language processing\n",
    "\n",
    "de_core_news_lg:  trained pipelines for German language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34f704ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-28 22:56:05.080543: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-28 22:56:05.080584: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "sp = spacy.load('de_core_news_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc67371",
   "metadata": {},
   "source": [
    "## Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0a8e6d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                  musikpersonal verwalten\n",
       "1                      strafvollzugsverfahr beaufsichtigen\n",
       "2                     nicht unterdrückend praktik anwenden\n",
       "3        einhaltung von vorschrift von eisenbahnfahrzeu...\n",
       "4                               verfügbar dienst ermitteln\n",
       "                               ...                        \n",
       "13886    beruflich leistungsfähigkeit von nutzer nutzer...\n",
       "13887               beleuchtung in transportgerät einbauen\n",
       "13888                       verarbeitung natürlich sprache\n",
       "13889                               bauarbeit koordinieren\n",
       "13890           absturzsicherung und bordbretter anbringen\n",
       "Name: processedLabel, Length: 13891, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_processed = pd.read_csv(\"../data/labels_processed.csv\")['processedLabel']\n",
    "labels_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dfa3961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        musikpersonal verwalten -- zuweisen verwalten ...\n",
       "1        strafvollzugsverfahr beaufsichtigen -- überwac...\n",
       "2        unterdrückend praktik anwenden -- ermitteln re...\n",
       "3        einhaltung vorschrift eisenbahnfahrzeuge überp...\n",
       "4        verfügbar dienst ermitteln -- ermitteln versch...\n",
       "                               ...                        \n",
       "13886    beruflich leistungsfähigkeit nutzer -- nutzeri...\n",
       "13887    beleuchtung transportgerät einbauen -- einbau ...\n",
       "13888    verarbeitung natürlich sprache -- technologie ...\n",
       "13889    bauarbeit koordinieren -- koordinierung tätigk...\n",
       "13890    absturzsicherung bordbretter anbringen -- anbr...\n",
       "Name: skill_info_processed, Length: 13891, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skills_info_processed = pd.read_csv(\"../data/skills_info_processed.csv\")['skill_info_processed']\n",
    "skills_info_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2bde99d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        -- schwierig -- klient -- -- patient -- angehö...\n",
       "1        aktuelles arbeitsrecht 2022 -- kurzbeschreibun...\n",
       "2        ambulant pflege -- rechtssicher handeln haftun...\n",
       "3        aufgabe gesetzlich betreuer -- reform betreuun...\n",
       "4        basisqualifikation ungelernt pflegekräft -- ze...\n",
       "                               ...                        \n",
       "16847    5 monat weiterbildung -- organisation & führun...\n",
       "16848    conversion usability experte -- ziel maßnahme ...\n",
       "16849    digital transformation management -- ziel maßn...\n",
       "16850    e-commerce geschäftsmodell -- ziel maßnahme te...\n",
       "16851    experte digital content creation -- teilnehmer...\n",
       "Name: course_info_processed, Length: 16852, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "courses_info_processed = pd.read_csv(\"../data/courses_info_processed.csv\")['course_info_processed']\n",
    "courses_info_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8ef5be",
   "metadata": {},
   "source": [
    "## NLP Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c245d0a",
   "metadata": {},
   "source": [
    "### 1. Modified Ontology-based Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf67a49",
   "metadata": {},
   "source": [
    "`termStore:  {controlled vocabulary (vocabularies in label): URI}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91eafb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "termStore = {}\n",
    "URI = 0\n",
    "for label_processed in labels_processed:\n",
    "    label_processed = sp(label_processed)\n",
    "    for word in label_processed:\n",
    "        word = str(word)\n",
    "        if word not in termStore:\n",
    "            termStore[word] = URI\n",
    "            URI += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256c69e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(termStore.items(), columns = ['controlledVocabulary', 'URI'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea015bf",
   "metadata": {},
   "source": [
    "`sequenceStore: {URIs : (index, sequence consisted of controlled vocabularies (label))}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1286455b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequenceStore = {}\n",
    "for i, label_processed in enumerate(labels_processed):\n",
    "    URIs = []\n",
    "    label_processed = sp(label_processed)\n",
    "    for word in label_processed:\n",
    "        URIs.append(termStore[str(word)])\n",
    "    sequenceStore[tuple(URIs)] = (i,str(label_processed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636ef1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(sequenceStore.items(), columns = ['URIs', '(index, label)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a81d555",
   "metadata": {},
   "source": [
    "The algorithm scans the tokenized courses information from the beginning until a word contained in the `termStore` is reached. Starting from this word a lookahead is performed searching for the longest sequence of words, which are contained in the `termStore`. As soon as a subsequent term is not included in the `termStore`, the `check_candidates` method to find all sequence still contained in the `sequenceStore` by using URIs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a277871c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relations_ER(index_start, index_end):\n",
    "    URIs_candidates = []\n",
    "    word_candidates = []\n",
    "    relations = []\n",
    "    courses_info_processed_subset = courses_info_processed[index_start:index_end]\n",
    "    for i, course_info_processed in enumerate(courses_info_processed_subset):\n",
    "        index_course = index_start + i\n",
    "        for word in sp(course_info_processed):\n",
    "            word = str(word)\n",
    "            if word != '--' and word in termStore:\n",
    "                word_candidates.append(word)\n",
    "                URIs_candidates.append(termStore[word])\n",
    "            else:\n",
    "                if URIs_candidates != []:\n",
    "                    URIs_candidates, relations = check_candidates(URIs_candidates, index_course, relations)\n",
    "                word_candidates = []\n",
    "            \n",
    "#             # approximate matching, results not good \n",
    "#             for key in termStore.keys():\n",
    "#                 if str(word) == key or key.startswith(str(word)):\n",
    "#                     word_candidates.append(key)\n",
    "#                     URIs_candidates.append(termStore[key])\n",
    "#                 else:\n",
    "#                     if URIs_candidates != []:\n",
    "#                         URIs_candidates, relations = check_candidates(URIs_candidates, index_course, relations)\n",
    "#                     word_candidates = []\n",
    "                    \n",
    "    URIs_candidates, relations = check_candidates(URIs_candidates, index_course, relations)\n",
    "    \n",
    "#     for relation in relations:\n",
    "#         print(courses['course_name'][relation[0]])\n",
    "#         print(' ---> ' + skills['preferredLabel'][relation[1]] + '   ' + skills['conceptUri'][relation[1]])\n",
    "#         print()\n",
    "    return relations\n",
    "\n",
    "        \n",
    "def check_candidates(URIs_candidates, index_course, relations):\n",
    "    n = len(URIs_candidates)\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n+1):\n",
    "            URIs = tuple(URIs_candidates[i:j])\n",
    "            if URIs in sequenceStore:\n",
    "                index_label = sequenceStore[URIs][0]\n",
    "                if (index_course, index_label) not in relations:\n",
    "                    relations.append((index_course, index_label))\n",
    "    URIs_candidates = []\n",
    "    return URIs_candidates, relations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5cfc13",
   "metadata": {},
   "source": [
    "### 2. Universal Sentence Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d141923d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import tensorflow_text\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5def728",
   "metadata": {},
   "source": [
    "#### Calculate skill embedings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "a3090b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_info_processed_embedings = []\n",
    "batch_size = 1000\n",
    "for i in range(0, len(skills_info_processed), batch_size):\n",
    "    skill_info_processed_embedings.extend(embed(skills_info_processed[i: i + batch_size]))\n",
    "skill_info_processed_embedings = np.array(skill_info_processed_embedings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a722a6c",
   "metadata": {},
   "source": [
    "#### Find course with related skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "1e713838",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relations_NN(index_start, index_end):\n",
    "    threshold = 0.45\n",
    "    top_n = 5\n",
    "    relations = []\n",
    "    courses_info_processed_subset = courses_info_processed[index_start:index_end]\n",
    "    for i, course_info_processed in enumerate(courses_info_processed_subset):\n",
    "        course_i = index_start + i\n",
    "        course_info_processed_embeding = embed(course_info_processed)\n",
    "        similarities = np.inner(course_info_processed_embeding, skill_info_processed_embedings)[0]\n",
    "        top_i = np.where(similarities >= threshold)[0]\n",
    "        top_similarities = similarities[similarities >= threshold]\n",
    "        related_skills_i_similarity = list(zip(top_i, top_similarities))\n",
    "        top_related_skills_i_similarity = sorted(related_skills_i_similarity, key = lambda x: x[1], reverse=True)[:top_n]\n",
    "        top_related_skills_i = list(map(lambda x: x[0], top_related_skills_i_similarity))\n",
    "        relations.extend(list(zip([course_i]*len(top_related_skills_i), top_related_skills_i)))\n",
    "    return relations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e4d9a4",
   "metadata": {},
   "source": [
    "### Calculate id relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "899bdb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'NN'\n",
    "start, end = 0, len(courses_info_processed)\n",
    "\n",
    "if model == 'ER':\n",
    "    # Modified Ontology-based Entity Recognition\n",
    "    relations = get_relations_ER(start, end)\n",
    "    file_name = 'all_relations_ER.csv'\n",
    "    \n",
    "elif model == 'NN':\n",
    "    # Universal Sentence Encoder\n",
    "    relations = get_relations_NN(start, end)\n",
    "    file_name = 'all_relations_NN.csv'\n",
    "    \n",
    "else: raise Exception(\"Please set model to ER or NN\")\n",
    "    \n",
    "graph = []\n",
    "for relation in relations:\n",
    "    graph.append((courses['course_id'][relation[0]],skills['conceptUri'][relation[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "33afd64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "graphp_df = pd.DataFrame(graph)\n",
    "graphp_df.columns =['course_id', 'concept_uri']\n",
    "graphp_df.to_csv(\"../data/{}\".format(file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11bcaa6",
   "metadata": {},
   "source": [
    "### Map id relations to name relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "6703729b",
   "metadata": {},
   "outputs": [],
   "source": [
    "courses = pd.read_csv('../data/all_courses.csv')[['course_id','course_name']]\n",
    "skills = pd.read_csv('../data/skills_de.csv')[['conceptUri','preferredLabel']]\n",
    "id_relations = pd.read_csv(\"../data/{}\".format(file_name)).iloc[:,1:3]\n",
    "id_relations.columns = ['course_id','conceptUri']\n",
    "skill_dict = skills.set_index('conceptUri').to_dict()['preferredLabel']\n",
    "course_dict = courses.set_index('course_id').to_dict()['course_name']\n",
    "name_relations = pd.DataFrame(columns=['course_name','skill_label'])\n",
    "name_relations['course_name'] = id_relations['course_id'].map(course_dict)\n",
    "name_relations['skill_label'] = id_relations['conceptUri'].map(skill_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "8a286ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>course_name</th>\n",
       "      <th>skill_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"Schwierige\" Klienten? - Mit Patienten, Angehö...</td>\n",
       "      <td>in der Fachkrankenpflege kommunizieren</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Schwierige\" Klienten? - Mit Patienten, Angehö...</td>\n",
       "      <td>Feedback zum Kommunikationsstil von Patienten/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"Schwierige\" Klienten? - Mit Patienten, Angehö...</td>\n",
       "      <td>auf Vorstellungsgespräch vorbereiten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Schwierige\" Klienten? - Mit Patienten, Angehö...</td>\n",
       "      <td>Fachkrankenpflege</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Schwierige\" Klienten? - Mit Patienten, Angehö...</td>\n",
       "      <td>Gespräche in den sozialen Diensten führen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44857</th>\n",
       "      <td>Digital Transformation Management</td>\n",
       "      <td>zu Überlegungen von Programmgestaltern/Program...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44858</th>\n",
       "      <td>Digital Transformation Management</td>\n",
       "      <td>beim digitalen Wandel industrieller Prozesse a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44859</th>\n",
       "      <td>Digital Transformation Management</td>\n",
       "      <td>innovative Mobilitätslösungen entwickeln</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44860</th>\n",
       "      <td>Experte im Digital Content Creation</td>\n",
       "      <td>zu Überlegungen von Programmgestaltern/Program...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44861</th>\n",
       "      <td>Experte im Digital Content Creation</td>\n",
       "      <td>digitales Lehrmaterial entwickeln</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44862 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             course_name  \\\n",
       "0      \"Schwierige\" Klienten? - Mit Patienten, Angehö...   \n",
       "1      \"Schwierige\" Klienten? - Mit Patienten, Angehö...   \n",
       "2      \"Schwierige\" Klienten? - Mit Patienten, Angehö...   \n",
       "3      \"Schwierige\" Klienten? - Mit Patienten, Angehö...   \n",
       "4      \"Schwierige\" Klienten? - Mit Patienten, Angehö...   \n",
       "...                                                  ...   \n",
       "44857                  Digital Transformation Management   \n",
       "44858                  Digital Transformation Management   \n",
       "44859                  Digital Transformation Management   \n",
       "44860                Experte im Digital Content Creation   \n",
       "44861                Experte im Digital Content Creation   \n",
       "\n",
       "                                             skill_label  \n",
       "0                 in der Fachkrankenpflege kommunizieren  \n",
       "1      Feedback zum Kommunikationsstil von Patienten/...  \n",
       "2                   auf Vorstellungsgespräch vorbereiten  \n",
       "3                                      Fachkrankenpflege  \n",
       "4              Gespräche in den sozialen Diensten führen  \n",
       "...                                                  ...  \n",
       "44857  zu Überlegungen von Programmgestaltern/Program...  \n",
       "44858  beim digitalen Wandel industrieller Prozesse a...  \n",
       "44859           innovative Mobilitätslösungen entwickeln  \n",
       "44860  zu Überlegungen von Programmgestaltern/Program...  \n",
       "44861                  digitales Lehrmaterial entwickeln  \n",
       "\n",
       "[44862 rows x 2 columns]"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_relations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
